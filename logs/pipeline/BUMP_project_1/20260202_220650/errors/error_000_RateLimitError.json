{
  "timestamp": "2026-02-02T22:06:50.606771",
  "error_type": "RateLimitError",
  "error_message": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jk5m9gdye45a18c9tq4z9e50` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99171, Requested 1556. Please try again in 10m28.128s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
  "traceback": "Traceback (most recent call last):\n  File \"D:\\FYP\\master\\AURA\\backend\\app\\agents\\service.py\", line 103, in process_repository\n    result = app.invoke(\n             ^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 579, in invoke\n    for chunk in self.stream(\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 615, in transform\n    for chunk in self._transform_stream_with_config(\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1881, in _transform_stream_with_config\n    chunk: Output = context.run(next, iterator)  # type: ignore\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 355, in _transform\n    _interrupt_or_proceed(done, inflight, step)\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 698, in _interrupt_or_proceed\n    raise exc\n  File \"C:\\Users\\ggthi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4525, in invoke\n    return self.bound.invoke(\n           ^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2499, in invoke\n    input = step.invoke(\n            ^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3963, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1626, in _call_with_config\n    context.run(\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 347, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3837, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 347, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\app\\agents\\workflow.py\", line 129, in call_model\n    response = llm.invoke(messages_with_tools)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 158, in invoke\n    self.generate_prompt(\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 421, in generate\n    raise e\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 411, in generate\n    self._generate_with_cache(\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 632, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py\", line 240, in _generate\n    response = self.client.create(messages=message_dicts, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 461, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FYP\\master\\AURA\\backend\\.venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n    raise self._make_status_error_from_response(err.response) from None\ngroq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jk5m9gdye45a18c9tq4z9e50` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99171, Requested 1556. Please try again in 10m28.128s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
}